# 基于注意力的残差神经网络运用于光片荧光显微镜图像条纹伪影消除

**中科院分子影像重点实验室, 复杂系统管理与控制国家重点实验室，自动化研究所**

**2022年1月1日/生物医学光学快报**

 

## 背景知识：

- 光片荧光显微镜 (LSFM)：

一种光学切片荧光显微技术，可用于对具有低光漂白响应的生物医学样品进行快速和高分辨率成像[1，2]。目前，LSFM被广泛应用于神经病学[3]、血管分析[4，5]和全身成像[6]。

条纹伪影出现原因分析：

由于激发光路中或样品内部的高吸收或散射结构（如杂质或气泡）引起的条纹伪影会导致 LSFM 应用中的图像质量下降 [7]。

[![ZK1-7-ME-W-DONNCUTF6-D.png](https://i.postimg.cc/CLwCsPZk/ZK1-7-ME-W-DONNCUTF6-D.png)](https://postimg.cc/ThHWRcJ3)

图 1

- 伪影去除方法分类：

已经开发了几种技术来消除LSFM图像中的条纹伪影，这些技术可以分为**硬件修改**和**基于图像的处理方式**[8]。改进后的LSFM系统在低亮度或高密度样品中仍可能存在条纹伪影（成像系统的限制）[6]。

 

- 基于图像的处理算法来消除条纹伪影：

**1**、传统方法：

Fehrenbach等人[9]将消除条纹视为一个复原问题，提出了去除平稳噪声(如以条纹形式表现的噪声)的变分平稳噪声去除器(VSNR)方法。

Münch等人[10]采用快速傅立叶变换和小波相结合的方法滤除条纹噪声。

Liang等人[11]设计了一种多向条纹去除器 (MDSR) 方法，该方法在非下采样轮廓波变换域中应用快速傅里叶变换来缩小条纹分量。

Pollatou等人[12]基于条纹伪影的位置、背景建模和光照校正，提出了一种新的拼接生物图像的去条纹方法。

**缺点：**尽管这些基于图像的算法可以有效地去除静止的条纹伪影，但去除出现在随机方向上的伪影仍然具有挑战性

**2**、结合深度学习的方法：

已有不少研究表明深度学习用于图像恢复、增强、分割以及重建等领域有良好的表现，但是结合深度学习的光学图像条纹伪影处理方法仍然有待探索（该论文尝试结合深度学习设计出一种有效的条纹伪影去除方法）。

 

## 论文方法介绍：

本论文采用**基于注意力机制**和**残差神经网络**的数据预处理模型来对图像质量进行把控。

- **网络主框架：**

如图2所示，该模型以U-Net网络作为主框架，左侧为输入图像（含条纹伪影），右侧为输出图像（去条纹伪影）。网络中跳跃连接的编解码器结构使得恢复的特征图能够聚合不同尺度的特征，从而更好地区分图像中的样本信息和条带伪影，获得理想的输出。通过通道间的跳跃连接来集成编码器和解码器中的特征。跳跃连接保留了更多的维度信息，使网络能够更好地学习抽象的全局特征和详细的局部特征。同时，在解码器部分，上采样会导致特征图边缘信息的丢失，该网络能够通过拼接整合来自编码器的特征来提取边缘特征。

[![P4-G-M-Z9-RGX0-W-5-F-SSJA.png](https://i.postimg.cc/K8wgwJY9/P4-G-M-Z9-RGX0-W-5-F-SSJA.png)](https://postimg.cc/bZ0J20kb)

图 2 :U-Net网络作为该模型的主框架，每层中间都有一个注意力模块

- **残差模块：**

本网络结合DRN模块，该模块期望输入(失真的图像)和输出(去条纹噪声的图像)具有相似的值和结构。此外，与普通CNN相比，残差网络包含的恒等映射可以防止梯度爆炸或消失问题，从而有利于更深层次网络的训练。因此，为了有效地将图像中的条纹噪声去除，该论文使用残差块来学习输入和目标输出之间的差异。如图3所示，残差块由两个卷积层、一个跳跃连接和一个校正的线性单元组成。

[![7-Z5-O-LX6-BBQ6-7-EG-MAUE.png](https://i.postimg.cc/MpMn7VY1/7-Z5-O-LX6-BBQ6-7-EG-MAUE.png)](https://postimg.cc/5XJ91HTt)

图 3 : 注意力模块由残差块（蓝色部分）和注意模型（红色部分）组成。

- **卷积注意力模型：**

为了提高模型在去条带任务中的性能，该论文引入了一个包含自注意力的注意模型，如图3所示。该注意模型可以通过为特征赋予不同的权重来增加网络的建模能力。具体地说，基于不同特征映射的重要性，该值被选择性地衰减或放大。已有的注意模型可分为通道注意和空间注意。通道注意力获得矢量v，表示每个通道的重要性。同时，空间注意力计算每个像素的重要性，以确定最重要的区域。在此，该论文将卷积注意力模型(CBAM)应用于本项目 (见图4) 。

[![AH3-MSIO-PHTJOMJ-7-D-456-P.png](https://i.postimg.cc/kM2N16wX/AH3-MSIO-PHTJOMJ-7-D-456-P.png)](https://postimg.cc/hhq7hGHH)

图 4 : 卷积注意力模型(CBAM)

- **损失函数设计：**

该论文的目标是在保留图像中有用的样本信息的同时消除条纹伪影。在设计损失函数中，该论文首先采用了平均绝对误差(MAE) 来比较输出图像和真值图之间的差异。MAE计算输出像素和地面真值之间的差值的平均值:

[<img src="https://i.postimg.cc/NFs3t80V/QM1-DHAH-V-DUSD9-N2-DGN45.png" alt="QM1-DHAH-V-DUSD9-N2-DGN45.png" style="zoom:80%;" />](https://postimg.cc/njWRYDBY)

其中***I***是模型的输出，***I'***是地面真值标签，H、W和C分别表示图像的高度、宽度和通道深度。像素级的差异有利于图像强度的恢复。然而，MAE经常忽略细节，例如图案、边缘以及一些细小的变化。

为了弥补上述的缺陷，该论文通过特征提取器比较输出图像和真值图之间的差异：

[<img src="https://i.postimg.cc/Hxszs6QQ/8-M-U-N-Q-S8-MO7-G-G.png" alt="8-M-U-N-Q-S8-MO7-G-G.png" style="zoom:80%;" />](https://postimg.cc/WDCg56Y3)

其中ϕ表示特征图。该模型使用了 VGG-19 网络 [13] 中的 conv4-3，并使用 ImageNet [14]作为特征提取器进行了预训练。这种特征级损失有助于保留内容信息和图像细节。最终的损失函数表示如下：

[<img src="https://i.postimg.cc/DfSDrzX1/H-XFWFEA-PV-NQW7-HZZ-I.png" alt="H-XFWFEA-PV-NQW7-HZZ-I.png"  />](https://postimg.cc/mhG8RBVr)

其中图像细节的系数为一个可根据实际情况调节大小的超参数。

 

## 评价指标：

1、 MAE 平均像素差异

[![0-9-B-CQ-YTFW-K2-SY1-LQ-Y.png](https://i.postimg.cc/7PX62mS5/0-9-B-CQ-YTFW-K2-SY1-LQ-Y.png)](https://postimg.cc/XXG4MKHW)

2、 PSNR峰值信噪比

[![6-S-19-N-H-1-LG3-ZY-25-O0.png](https://i.postimg.cc/yd8q4XxG/6-S-19-N-H-1-LG3-ZY-25-O0.png)](https://postimg.cc/TyFNnbrj)

3、 SSIM结构相似性

[![6-O3-J-G-ERM-HR-X-ZAOPF.png](https://i.postimg.cc/ncbJMmPX/6-O3-J-G-ERM-HR-X-ZAOPF.png)](https://postimg.cc/grD5Txdp)（其中μ表示图像强度平均值，σ表示图像标准差）

其中MAE 计算真值图和输出图像之间的平均像素差异。因此，MAE 越小，ground truth和输出之间的误差就越小。 PSNR 和 SSIM 常用于图像恢复，更注重图像的保真度。

 

## 数据集制作：

为了验证网络的性能，论文设计了一个**退化模型**来生成各种条纹伪影，以构建伪影失真图像数据集。论文模拟了各种类型的条纹伪影，包括**规则的水平条纹**、**各向异性的高斯水平条纹**和**各向异性的高斯非水平条纹**。论文将经过筛选后成像质量较好的图像作为真值图，以及对应的通过退化模型得到的失真伪影图像用来训练该网络。

[![V5-PZ-7-NYVSY-94-SDWU-F-C3.png](https://i.postimg.cc/Ls7WwKdJ/V5-PZ-7-NYVSY-94-SDWU-F-C3.png)](https://postimg.cc/KKPQ3W8b)

图 5 数据集构造的工作流程

## 消融实验：

[![PF-GPJ-6-QTV-R86-PUVS6-P1.png](https://i.postimg.cc/HkkPLsdJ/PF-GPJ-6-QTV-R86-PUVS6-P1.png)](https://postimg.cc/SJHGT4Xq)

## 实验结果对比：

[![V-PXNN-ZJ2-Z-HWI-ILC6.png](https://i.postimg.cc/PNsTjpB8/V-PXNN-ZJ2-Z-HWI-ILC6.png)](https://postimg.cc/XB8TKJNV)

[![3-RH6-LM33-FX70-V5-4-3-UTDY.png](https://i.postimg.cc/Njt2855M/3-RH6-LM33-FX70-V5-4-3-UTDY.png)](https://postimg.cc/hfCGc48W)

[![0-P-LGON3-ZQCG-W-R-7-GXDN.png](https://i.postimg.cc/HkTjcRh8/0-P-LGON3-ZQCG-W-R-7-GXDN.png)](https://postimg.cc/56kfD71f)

[![YDJ2-Y-HU3-DNIS52-RT3-Q-P1.png](https://i.postimg.cc/KYzzQLFZ/YDJ2-Y-HU3-DNIS52-RT3-Q-P1.png)](https://postimg.cc/tsKp4sYf)

 

##  思考：

- 后续工作和这个有结合不？是想要把这个应用到我们这块的预处理中的去除伪影步骤吗？

- 当前我们这边存在这样的问题吗？

- 它最终的效果存在什么问题？

- 模拟的条纹伪影和图片中真实存在的条纹伪影的相似性如何衡量？是否只是加性；



## 参考文献：

1. R. M. Power and J. Huisken, “A guide to light-sheet fluorescence microscopy for multiscale imaging,” Nat. Methods14(4), 360–373 (2017).
2. T. Chakraborty, M. K. Driscoll, E. Jeffery, M. M. Murphy, P. Roudot, B. J. Chang, S. Vora, W. M. Wong, C. D.Nielson, H. Zhang, V. Zhemkov, C. Hiremath, E. D. De la Cruz, Y. T. Yi, I. Bezprozvanny, H. Zhao, R. Tomer, R.Heintzmann, J. P. Meeks, D. K. Marciano, S. J. Morrison, G. Danuser, K. M. Dean, and R. Fiolka, “Light-sheet microscopy of cleared tissues with isotropic, subcellular resolution,” Nat. Methods 16(11), 1109–1113 (2019).
3. E. A. Susaki, K. Tainaka, D. Perrin, F. Kishino, T. Tawara, T. M. Watanabe, C. Yokoyama, H. Onoe, M. Eguchi, S.Yamaguchi, T. Abe, H. Kiyonari, Y.Shimizu, A. Miyawaki, H. Yokota, and H. R. Ueda, “Whole-brain imaging with single-cell resolution using chemical cocktails and computational analysis,” Cell 157(3), 726–739 (2014).
4. R. Y. Cai, C. C. Pan, A. Ghasemigharagoz, M. I. Todorov, B. Forstera, S. Zhao, H. S. Bhatia, A. Parra-Damas, L.Mrowka, D. Theodorou, M. Rempfler, A. L. R. Xavier, B. T. Kress, C. Benakis, H. Steinke, S. Liebscher, I. Bechmann,A. Liesz, B. Menze, M. Kerschensteiner, M. Nedergaard, and A. Erturk, “Panoptic imaging of transparent mice reveals whole-body neuronal projections and skull-meninges connections,” Nat. Neurosci. 22(2), 317–327 (2019).
5. W. Wang, Y. Q. Zhang, H. Hui, W. Tong, Z. C. Wei, Z. X. Li, S. H. Zhang, X. Yang, J. Tian, and Y. D. Chen, “The effect of endothelial progenitor cell transplantation on neointimal hyperplasia and reendothelialisation after balloon catheter injury in rat carotid arteries,” Stem Cell Res Ther 12(1), 9 (2021).
6. K. Tainaka, S. I. Kubota, T. Q. Suyama, E. A. Susaki, D. Perrin, M. Ukai-Tadenuma, H. Ukai, and H. R. Ueda, “Whole-body imaging with single-cell resolution by tissue decolorization,” Cell 159(4), 911–924 (2014).
7. A. Rohrbach, “Artifacts resulting from imaging in scattering media: a theoretical prediction,” Opt. Lett. 34 (19), 3041–3043 (2009).
8. P. Ricci, V. Gavryusev, C. Müllenbroich, L. Turrini, G. de Vito, L. Silvestri, G. Sancataldo, and F. S. Pavone, “Removing striping artifacts in light-sheet fluorescence microscopy: A review,” Prog. Biophys. Mol. Biol. 168 , 52–65 (2021).
9. J. Fehrenbach, P. Weiss, and C. Lorenzo, “Variational algorithms to remove stationary noise: applications to microscopy imaging,” IEEE Trans. on Image Process. 21(10), 4420–4430 (2012).
10. B. Munch, P. Trtik, F. Marone, and M. Stampanoni, “Stripe and ring artifact removal with combined wavelet - Fourier filtering,” Opt. Express 17(10), 8567–8591 (2009).
11. X. Liang, Y. Zang, D. Dong, L. W. Zhang, M. J. Fang, X. Yang, A. Arranz, J. Ripoll, H. Hui, and J. Tian, “Stripe artifact elimination based on nonsubsampled contourlet transform for light sheet fluorescence microscopy,” J. Biomed. Opt. 21(10), 106005 (2016).
12. A. Pollatou, “An automated method for removal of striping artifacts in fluorescent whole-slide microscopy,” J Neurosci Meth 341, 108781 (2020).
13. K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv:1409.1556 (2014).
14. J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and F. F. Li, “ImageNet: a large-scale hierarchical image database,” in IEEE Conference on Computer Vision and Pattern Recognition (Springer, 2009), pp. 248–255.

